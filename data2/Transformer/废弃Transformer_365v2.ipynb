{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from model import LSTMModel,BiLSTM_FullOutput\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"../../train_hourly.csv\"\n",
    "test_data_path = \"../../test_hourly.csv\"\n",
    "train_df = pd.read_csv(train_data_path, index_col='DateTime', parse_dates=True)\n",
    "test_df = pd.read_csv(test_data_path, index_col='DateTime', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据维度: (17911, 12)\n",
      "测试数据维度: (16678, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import joblib # 用于保存scaler\n",
    "\n",
    "\n",
    "train_scaled = train_df.values.astype(np.float32)  \n",
    "test_scaled = test_df.values.astype(np.float32)\n",
    "\n",
    "\n",
    "print(f\"训练数据维度: {train_scaled.shape}\")\n",
    "print(f\"测试数据维度: {test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 长期预测 (90->365) 样本维度 ---\n",
      "X_train_long shape: (6992, 2160, 12)\n",
      "y_train_long shape: (6992, 8760)\n",
      "X_test_long shape: (5759, 2160, 12)\n",
      "y_test_long shape: (5759, 8760)\n"
     ]
    }
   ],
   "source": [
    "def create_sliding_windows(data, input_seq_len, output_seq_len, target_col_index):\n",
    "    X, y = [], []\n",
    "    n_samples = len(data)\n",
    "    # 确保我们有足够的数据来创建至少一个窗口\n",
    "    if n_samples < input_seq_len + output_seq_len:\n",
    "        return np.array(X), np.array(y)\n",
    "        \n",
    "    for i in range(n_samples - input_seq_len - output_seq_len + 1):\n",
    "        # 输入序列 (所有特征)\n",
    "        input_window = data[i : i + input_seq_len, :]\n",
    "        X.append(input_window)\n",
    "        \n",
    "        # 输出序列 (只包含目标特征)\n",
    "        output_window = data[i + input_seq_len : i + input_seq_len + output_seq_len, target_col_index]\n",
    "        y.append(output_window)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 定义常量\n",
    "INPUT_SEQ_LEN = 90*24\n",
    "OUTPUT_SEQ_LEN = 365*24\n",
    "TARGET_COL_IDX = train_df.columns.get_loc('Global_active_power')\n",
    "\n",
    "\n",
    "# --- 创建长期预测 (90 -> 365) 的训练和测试样本 ---\n",
    "X_train_long, y_train_long = create_sliding_windows(train_scaled, INPUT_SEQ_LEN, OUTPUT_SEQ_LEN, TARGET_COL_IDX)\n",
    "X_test_long, y_test_long = create_sliding_windows(test_scaled, INPUT_SEQ_LEN, OUTPUT_SEQ_LEN, TARGET_COL_IDX)\n",
    "\n",
    "print(\"\\n--- 长期预测 (90->365) 样本维度 ---\")\n",
    "print(f\"X_train_long shape: {X_train_long.shape}\")\n",
    "print(f\"y_train_long shape: {y_train_long.shape}\")\n",
    "print(f\"X_test_long shape: {X_test_long.shape}\")\n",
    "print(f\"y_test_long shape: {y_test_long.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# 检查是否有可用的GPU，并设置设备\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # 将Numpy数组转换为PyTorch张量\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)  # 保持二维 [N, output_seq_len]\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# 创建训练集和测试集实例\n",
    "train_dataset_long = TimeSeriesDataset(X_train_long, y_train_long)\n",
    "test_dataset_long = TimeSeriesDataset(X_test_long, y_test_long)\n",
    "\n",
    "# 创建数据加载器\n",
    "BATCH_SIZE = 128\n",
    "train_loader_long = DataLoader(train_dataset_long, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_long = DataLoader(test_dataset_long, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1)].unsqueeze(0).to(x.device)\n",
    "\n",
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
    "                 input_seq_len, output_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.target_proj = nn.Linear(1, d_model)\n",
    "\n",
    "        self.pos_enc_input = PositionalEncoding(d_model, max_len=input_seq_len)\n",
    "        self.pos_enc_target = PositionalEncoding(d_model, max_len=output_seq_len)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, d_model * 2, dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, d_model * 2, dropout, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.input_proj(src)  # [B, input_seq_len, d_model]\n",
    "        src = self.pos_enc_input(src)\n",
    "        memory = self.encoder(src)\n",
    "\n",
    "        tgt = self.target_proj(tgt)  # [B, output_seq_len, d_model]\n",
    "        tgt = self.pos_enc_target(tgt)\n",
    "\n",
    "        output = self.decoder(tgt, memory)\n",
    "        output = self.output_layer(output).squeeze(-1)  # [B, output_seq_len]\n",
    "        return output\n",
    "\n",
    "\n",
    "INPUT_DIM=12\n",
    "INPUT_SEQ_LEN=90*24\n",
    "OUTPUT_SEQ_LEN=365*24\n",
    "D_MODEL=128\n",
    "NHEAD=8\n",
    "NUM_LAYERS=2\n",
    "DROPOUT=0.3\n",
    "\n",
    "# transformer_model_long = TransformerModel(INPUT_DIM,D_MODEL,NHEAD,NUM_LAYERS,OUTPUT_SEQ_LEN,DROPOUT,INPUT_SEQ_LEN).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        decoder_input = torch.zeros(X_batch.size(0), OUTPUT_SEQ_LEN, 1).to(device)\n",
    "\n",
    "        outputs = model(X_batch, decoder_input)  # 传入 decoder_input\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            decoder_input = torch.zeros(X_batch.size(0), OUTPUT_SEQ_LEN, 1).to(device)\n",
    "\n",
    "            outputs = model(X_batch, decoder_input)  # 传入 decoder_input\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "            \n",
    "    epoch_loss = running_loss / len(test_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def get_predictions(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            decoder_input = torch.zeros(X_batch.size(0), OUTPUT_SEQ_LEN, 1).to(device)\n",
    "            outputs = model(X_batch, decoder_input).cpu()\n",
    "\n",
    "            predictions.append(outputs.numpy())\n",
    "            actuals.append(y_batch.numpy())\n",
    "    return np.concatenate(predictions), np.concatenate(actuals)\n",
    "\n",
    "def inverse_transform_data(scaled_data, scaler, target_col_idx):\n",
    "    # scaled_data shape: (num_samples, seq_len)\n",
    "    num_samples = scaled_data.shape[0]\n",
    "    num_features = scaler.n_features_in_\n",
    "    \n",
    "    # 创建一个 (num_samples * seq_len, num_features) 的零矩阵\n",
    "    dummy_array = np.zeros((num_samples * scaled_data.shape[1], num_features))\n",
    "    # 将我们的数据放入目标列\n",
    "    dummy_array[:, target_col_idx] = scaled_data.flatten()\n",
    "    \n",
    "    # 执行反归一化\n",
    "    inversed_data_flat = scaler.inverse_transform(dummy_array)[:, target_col_idx]\n",
    "    \n",
    "    # 重新塑形为 (num_samples, seq_len)\n",
    "    return inversed_data_flat.reshape(num_samples, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " LSTM Long-term Prediction (90 -> 365) \n",
      "==============================\n",
      "\n",
      "--- 实验轮次: 1/5 (Long-term) ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 2. 训练 (使用长期数据加载器)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# 注意这里，我们使用 train_loader_long 和 test_loader_long\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_long\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader_long, criterion, device)\n\u001b[1;32m     37\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(test_loss)\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 16\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*30 + \"\\n LSTM Long-term Prediction (90 -> 365) \\n\" + \"=\"*30)\n",
    "\n",
    "NUM_EXPERIMENTS = 5\n",
    "NUM_EPOCHS = 100 # 对于更难的长期任务，可能需要更多轮次\n",
    "\n",
    "all_mse_scores_long = []\n",
    "all_mae_scores_long = []\n",
    "best_long_model_state = None\n",
    "best_long_model_loss = float('inf')\n",
    "\n",
    "for i in range(NUM_EXPERIMENTS):\n",
    "    print(f\"\\n--- 实验轮次: {i+1}/{NUM_EXPERIMENTS} (Long-term) ---\")\n",
    "    \n",
    "    # 1. 准备模型 (使用长期预测的参数)\n",
    "    torch.manual_seed(42 + i)\n",
    "    # 注意这里，我们使用 OUTPUT_WINDOW_LONG\n",
    "    model = TransformerEncoderDecoder(\n",
    "        input_dim=INPUT_DIM,\n",
    "        d_model=D_MODEL,\n",
    "        nhead=NHEAD,\n",
    "        num_encoder_layers=NUM_LAYERS,\n",
    "        num_decoder_layers=NUM_LAYERS,\n",
    "        input_seq_len=INPUT_SEQ_LEN,\n",
    "        output_seq_len=OUTPUT_SEQ_LEN,\n",
    "        dropout=DROPOUT\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5) #耐心可以适当增加\n",
    "\n",
    "    # 2. 训练 (使用长期数据加载器)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # 注意这里，我们使用 train_loader_long 和 test_loader_long\n",
    "        train_loss = train_model(model, train_loader_long, criterion, optimizer, device)\n",
    "        test_loss = evaluate_model(model, test_loader_long, criterion, device)\n",
    "        scheduler.step(test_loss)\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'  Epoch [{epoch+1:02d}/{NUM_EPOCHS}], Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}')\n",
    "    \n",
    "    # 保存性能最好的模型\n",
    "    if test_loss < best_long_model_loss:\n",
    "        best_long_model_loss = test_loss\n",
    "        best_long_model_state = model.state_dict()\n",
    "\n",
    "    # 3. 评估\n",
    "    predictions_scaled, actuals_scaled = get_predictions(model, test_loader_long, device)\n",
    "    predictions = predictions_scaled\n",
    "    actuals = actuals_scaled\n",
    "    mse = mean_squared_error(actuals.flatten(), predictions.flatten())\n",
    "    mae = mean_absolute_error(actuals.flatten(), predictions.flatten())\n",
    "    all_mse_scores_long.append(mse)\n",
    "    all_mae_scores_long.append(mae)\n",
    "    print(f\"  实验 {i+1} 结束. MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "# 打印长期预测的最终统计结果\n",
    "mean_mse_long = np.mean(all_mse_scores_long)\n",
    "std_mse_long = np.std(all_mse_scores_long)\n",
    "mean_mae_long = np.mean(all_mae_scores_long)\n",
    "std_mae_long = np.std(all_mae_scores_long)\n",
    "print(f\"\\n长期预测最终结果: Avg MSE: {mean_mse_long:.4f} (±{std_mse_long:.4f}), Avg MAE: {mean_mae_long:.4f} (±{std_mae_long:.4f})\")\n",
    "\n",
    "\n",
    "# --- 可视化长期预测 (使用性能最好的模型) ---\n",
    "# 1. 加载最佳模型\n",
    "vis_model_long = TransformerEncoderDecoder(\n",
    "        input_dim=INPUT_DIM,\n",
    "        d_model=D_MODEL,\n",
    "        nhead=NHEAD,\n",
    "        num_encoder_layers=NUM_LAYERS,\n",
    "        num_decoder_layers=NUM_LAYERS,\n",
    "        input_seq_len=INPUT_SEQ_LEN,\n",
    "        output_seq_len=OUTPUT_SEQ_LEN,\n",
    "        dropout=DROPOUT\n",
    "    ).to(device)\n",
    "vis_model_long.load_state_dict(best_long_model_state)\n",
    "# 2. 获取预测结果\n",
    "predictions_scaled, actuals_scaled = get_predictions(vis_model_long, test_loader_long, device)\n",
    "predictions = predictions_scaled\n",
    "actuals = actuals_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 绘图\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(actuals[0, :366], label='Ground Truth')\n",
    "plt.plot(predictions[0, :366], label='Transformer Prediction')\n",
    "plt.title('Transformer: 365-Day Power Consumption Prediction')\n",
    "plt.xlabel('Days into the Future')\n",
    "plt.ylabel('Daily Global Active Power (Sum)')\n",
    "\n",
    "plt.xticks(np.arange(0, 366, 30), rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
